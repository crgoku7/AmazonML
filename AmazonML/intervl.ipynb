{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from decord import VideoReader, cpu\n",
    "from PIL import Image\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "IMAGENET_MEAN = (0.485, 0.456, 0.406)\n",
    "IMAGENET_STD = (0.229, 0.224, 0.225)\n",
    "\n",
    "def build_transform(input_size):\n",
    "    MEAN, STD = IMAGENET_MEAN, IMAGENET_STD\n",
    "    transform = T.Compose([\n",
    "        T.Lambda(lambda img: img.convert('RGB') if img.mode != 'RGB' else img),\n",
    "        T.Resize((input_size, input_size), interpolation=InterpolationMode.BICUBIC),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=MEAN, std=STD)\n",
    "    ])\n",
    "    return transform\n",
    "\n",
    "def find_closest_aspect_ratio(aspect_ratio, target_ratios, width, height, image_size):\n",
    "    best_ratio_diff = float('inf')\n",
    "    best_ratio = (1, 1)\n",
    "    area = width * height\n",
    "    for ratio in target_ratios:\n",
    "        target_aspect_ratio = ratio[0] / ratio[1]\n",
    "        ratio_diff = abs(aspect_ratio - target_aspect_ratio)\n",
    "        if ratio_diff < best_ratio_diff:\n",
    "            best_ratio_diff = ratio_diff\n",
    "            best_ratio = ratio\n",
    "        elif ratio_diff == best_ratio_diff:\n",
    "            if area > 0.5 * image_size * image_size * ratio[0] * ratio[1]:\n",
    "                best_ratio = ratio\n",
    "    return best_ratio\n",
    "\n",
    "def dynamic_preprocess(image, min_num=1, max_num=12, image_size=448, use_thumbnail=False):\n",
    "    orig_width, orig_height = image.size\n",
    "    aspect_ratio = orig_width / orig_height\n",
    "\n",
    "    # calculate the existing image aspect ratio\n",
    "    target_ratios = set(\n",
    "        (i, j) for n in range(min_num, max_num + 1) for i in range(1, n + 1) for j in range(1, n + 1) if\n",
    "        i * j <= max_num and i * j >= min_num)\n",
    "    target_ratios = sorted(target_ratios, key=lambda x: x[0] * x[1])\n",
    "\n",
    "    # find the closest aspect ratio to the target\n",
    "    target_aspect_ratio = find_closest_aspect_ratio(\n",
    "        aspect_ratio, target_ratios, orig_width, orig_height, image_size)\n",
    "\n",
    "    # calculate the target width and height\n",
    "    target_width = image_size * target_aspect_ratio[0]\n",
    "    target_height = image_size * target_aspect_ratio[1]\n",
    "    blocks = target_aspect_ratio[0] * target_aspect_ratio[1]\n",
    "\n",
    "    # resize the image\n",
    "    resized_img = image.resize((target_width, target_height))\n",
    "    processed_images = []\n",
    "    for i in range(blocks):\n",
    "        box = (\n",
    "            (i % (target_width // image_size)) * image_size,\n",
    "            (i // (target_width // image_size)) * image_size,\n",
    "            ((i % (target_width // image_size)) + 1) * image_size,\n",
    "            ((i // (target_width // image_size)) + 1) * image_size\n",
    "        )\n",
    "        # split the image\n",
    "        split_img = resized_img.crop(box)\n",
    "        processed_images.append(split_img)\n",
    "    assert len(processed_images) == blocks\n",
    "    if use_thumbnail and len(processed_images) != 1:\n",
    "        thumbnail_img = image.resize((image_size, image_size))\n",
    "        processed_images.append(thumbnail_img)\n",
    "    return processed_images\n",
    "\n",
    "def load_image(image_file, input_size=448, max_num=12):\n",
    "    image = Image.open(image_file).convert('RGB')\n",
    "    transform = build_transform(input_size=input_size)\n",
    "    images = dynamic_preprocess(image, image_size=input_size, use_thumbnail=True, max_num=max_num)\n",
    "    pixel_values = [transform(image) for image in images]\n",
    "    pixel_values = torch.stack(pixel_values)\n",
    "    return pixel_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "def split_model(model_name):\n",
    "    device_map = {}\n",
    "    print(torch.cuda .is_available())\n",
    "    world_size = torch.cuda.device_count()\n",
    "    print(world_size)\n",
    "    num_layers = {\n",
    "        'InternVL2-1B': 24, 'InternVL2-2B': 24, 'InternVL2-4B': 32, 'InternVL2-8B': 32,\n",
    "        'InternVL2-26B': 48, 'InternVL2-40B': 60, 'InternVL2-Llama3-76B': 80}[model_name]\n",
    "    # Since the first GPU will be used for ViT, treat it as half a GPU.\n",
    "    num_layers_per_gpu = math.ceil(num_layers / (world_size - 0.5))\n",
    "    num_layers_per_gpu = [num_layers_per_gpu] * world_size\n",
    "    num_layers_per_gpu[0] = math.ceil(num_layers_per_gpu[0] * 0.5)\n",
    "    layer_cnt = 0\n",
    "    for i, num_layer in enumerate(num_layers_per_gpu):\n",
    "        for j in range(num_layer):\n",
    "            device_map[f'language_model.model.layers.{layer_cnt}'] = i\n",
    "            layer_cnt += 1\n",
    "    device_map['vision_model'] = 0\n",
    "    device_map['mlp1'] = 0\n",
    "    device_map['language_model.model.tok_embeddings'] = 0\n",
    "    device_map['language_model.model.embed_tokens'] = 0\n",
    "    device_map['language_model.output'] = 0\n",
    "    device_map['language_model.model.norm'] = 0\n",
    "    device_map['language_model.lm_head'] = 0\n",
    "    device_map[f'language_model.model.layers.{num_layers - 1}'] = 0\n",
    "\n",
    "    return device_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n",
      "Warning: Flash attention is not available, using eager attention instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "path = \"pretrained/InternVL2-2B\"\n",
    "device_map = split_model('InternVL2-2B')\n",
    "model = AutoModel.from_pretrained(\n",
    "    path,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    load_in_8bit=True,\n",
    "    low_cpu_mem_usage=True,\n",
    "    use_flash_attn=True,\n",
    "    trust_remote_code=True,\n",
    "    device_map=device_map).eval()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(path, trust_remote_code=True, use_fast=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 3, 448, 448])\n"
     ]
    }
   ],
   "source": [
    "# set the max number of tiles in `max_num`\n",
    "pixel_values = load_image('./images/3.jpg', max_num=12, input_size=448).to(torch.bfloat16).cuda()\n",
    "generation_config = dict(max_new_tokens=32, do_sample=False)\n",
    "print(pixel_values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: <image>\n",
      "in the given image, from the text present in the image output only the numeric weight with units, do not perform any arithmetic operations on the numbers present in the image, just return the final answer, the answer may be a range of values like a-b <unit> so answer accordingly.\n",
      "Assistant: The numeric weight with units in the image is 17.21 kg.\n"
     ]
    }
   ],
   "source": [
    "# single-image single-round conversation (单图单轮对话)\n",
    "question = '<image>\\nin the given image there is a product, find the height of the product from the text data available in the image, only use the text data from the image, the answer is written in a straighforward manner and there is no need to do arithmetic on the different numbers present in the image.'\n",
    "question = '<image>\\nin the given image, from the text present in the image output only the numeric weight with units, do not perform any arithmetic operations on the numbers present in the image, just return the final answer, the answer may be a range of values like a-b <unit> so answer accordingly.'\n",
    "response = model.chat(tokenizer, pixel_values, question, generation_config)\n",
    "print(f'User: {question}\\nAssistant: {response}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_unit_map = {\n",
    "  \"width\": {\n",
    "    \"centimetre\",\n",
    "    \"foot\",\n",
    "    \"millimetre\",\n",
    "    \"metre\",\n",
    "    \"inch\",\n",
    "    \"yard\"\n",
    "  },\n",
    "  \"depth\": {\n",
    "    \"centimetre\",\n",
    "    \"foot\",\n",
    "    \"millimetre\",\n",
    "    \"metre\",\n",
    "    \"inch\",\n",
    "    \"yard\"\n",
    "  },\n",
    "  \"height\": {\n",
    "    \"centimetre\",\n",
    "    \"foot\",\n",
    "    \"millimetre\",\n",
    "    \"metre\",\n",
    "    \"inch\",\n",
    "    \"yard\"\n",
    "  },\n",
    "  \"item_weight\": {\n",
    "    \"milligram\",\n",
    "    \"kilogram\",\n",
    "    \"microgram\",\n",
    "    \"gram\",\n",
    "    \"ounce\",\n",
    "    \"ton\",\n",
    "    \"pound\"\n",
    "  },\n",
    "  \"maximum_weight_recommendation\": {\n",
    "    \"milligram\",\n",
    "    \"kilogram\",\n",
    "    \"microgram\",\n",
    "    \"gram\",\n",
    "    \"ounce\",\n",
    "    \"ton\",\n",
    "    \"pound\"\n",
    "  },\n",
    "  \"voltage\": {\n",
    "    \"millivolt\",\n",
    "    \"kilovolt\",\n",
    "    \"volt\"\n",
    "  },\n",
    "  \"wattage\": {\n",
    "    \"kilowatt\",\n",
    "    \"watt\"\n",
    "  },\n",
    "  \"item_volume\": {\n",
    "    \"cubic foot\",\n",
    "    \"microlitre\",\n",
    "    \"cup\",\n",
    "    \"fluid ounce\",\n",
    "    \"centilitre\",\n",
    "    \"imperial gallon\",\n",
    "    \"pint\",\n",
    "    \"decilitre\",\n",
    "    \"litre\",\n",
    "    \"millilitre\",\n",
    "    \"quart\",\n",
    "    \"cubic inch\",\n",
    "    \"gallon\"\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Path to the folder where all images are stored\n",
    "image_folder = \"path/to/the/folder\"  # Adjust this path based on where the images are stored\n",
    "\n",
    "# Lists to store image paths and query_entities\n",
    "image_paths = []\n",
    "query_entities = []\n",
    "\n",
    "# Read the CSV file using pandas\n",
    "csv_file = 'path/to/the/csvfile.csv'  # Replace with your actual CSV file name\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# Iterate over the DataFrame rows\n",
    "for index, row in df.iterrows():\n",
    "    image_index = row[0]  # First column: index\n",
    "    query_entity = row[3]  # Third column: query_entity\n",
    "\n",
    "    # Construct the image filename using the index (e.g., index.jpg)\n",
    "    image_name = f\"{image_index}.jpg\"\n",
    "    \n",
    "    # Construct the full image path (all images are in a single folder)\n",
    "    image_path = os.path.join(image_folder, image_name)\n",
    "    \n",
    "    if os.path.exists(image_path):  # Check if the image exists in the folder\n",
    "        image_paths.append(image_path)  # Add the image path to the list\n",
    "        query_entities.append(query_entity)  # Add the query_entity to the list\n",
    "    else:\n",
    "        print(f\"Image {image_name} not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = dict(max_new_tokens=32, do_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt_type = 'none'\n",
    "prompts = {\n",
    "    \"width\": f\"<image>\\n In the given image, find the width of the product using only the numerical text present in the image, the units are {{prompt_type}}, output only the number with it unit, the answer may be a range. \",\n",
    "    \"depth\": f\"<image>\\n In the given image, find the depth of the product using only the numerical text present in the image, the units are {{prompt_type}}, output only the number with it unit, the answer may be a range. \",\n",
    "    \"height\": f\"<image>\\n In the given image, find the height of the product using only the numerical text present in the image, the units are {{prompt_type}}, output only the number with it unit, the answer may be a range. \",\n",
    "    \"item_weight\": f\"<image>\\n In the given image, find the item_weight of the product using only the numerical text present in the image, the units are {{prompt_type}}, output only the number with it unit, the answer may be a range. \",\n",
    "    \"maximum_weight_recommendation\": f\"<image>\\n In the given image, find the maximum_weight_recommendation of the product using only the numerical text present in the image, the units are {{prompt_type}}, output only the number with it unit, the answer may be a range. \",\n",
    "    \"voltage\": f\"<image>\\n In the given image, find the voltage of the product using only the numerical text present in the image, the units are {{prompt_type}}, output only the number with it unit, the answer may be a range. \",\n",
    "    \"wattage\": f\"<image>\\n In the given image, find the wattage of the product using only the numerical text present in the image, the units are {{prompt_type}}, output only the number with it unit, the answer may be a range. \",\n",
    "    \"item_volume\": f\"<image>\\n In the given image, find the item_volume of the product using only the numerical text present in the image, the units are {{prompt_type}}, output only the number with it unit, the answer may be a range. \"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses_dict = {}\n",
    "\n",
    "for image_path, query_entity in zip(image_paths, query_entities):\n",
    "    # Finding the image name from image path\n",
    "    image_name = os.path.basename(image_path).split('.')[0]\n",
    "    # set the max number of tiles in `max_num`\n",
    "    pixel_values = load_image(image_path, max_num=12, input_size=448).to(torch.bfloat16).cuda()\n",
    "    prompt_type = entity_unit_map[query_entity]\n",
    "    # Defining the question\n",
    "    question = prompts[query_entity].replace('{{prompt_entity}}', prompt_type)\n",
    "    # Getting the model's response\n",
    "    response = model.chat(tokenizer, pixel_values, question, generation_config)\n",
    "    # Storing the model's response in a dictionary\n",
    "    responses_dict[image_name] = response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
